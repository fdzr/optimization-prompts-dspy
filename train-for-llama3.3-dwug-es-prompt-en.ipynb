{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2205aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Literal, Union\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from programs import WrapperEnglishSPT, evaluate_answer\n",
    "from custom_evaluation import custom_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"ollama_chat/llama3.3\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f38eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"what is your name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dev_dwug_es.csv\")\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dc88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    training_set.append(\n",
    "        dspy.Example(\n",
    "            sentence1=row[\"context_x\"],\n",
    "            sentence2=row[\"context_y\"],\n",
    "            target_word=row[\"lemma\"],\n",
    "            answer=int(row[\"judgment\"]),\n",
    "        ).with_inputs(\"sentence1\", \"sentence2\", \"target_word\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf95a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_1_es = [item for item in training_set if item.answer == 1]\n",
    "classes_2_es = [item for item in training_set if item.answer == 2]\n",
    "classes_3_es = [item for item in training_set if item.answer == 3]\n",
    "classes_4_es = [item for item in training_set if item.answer == 4]\n",
    "\n",
    "print(len(classes_1_es))\n",
    "print(len(classes_2_es))\n",
    "print(len(classes_3_es))\n",
    "print(len(classes_4_es))\n",
    "\n",
    "classes_1_train, classes_1_dev = train_test_split(\n",
    "    classes_1_es,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "classes_1_train, classes_1_test = train_test_split(\n",
    "    classes_1_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "classes_2_train, classes_2_dev = train_test_split(\n",
    "    classes_2_es,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_2_train, classes_2_test = train_test_split(\n",
    "    classes_2_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "classes_3_train, classes_3_dev = train_test_split(\n",
    "    classes_3_es,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_3_train, classes_3_test = train_test_split(\n",
    "    classes_3_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "classes_4_train, classes_4_dev = train_test_split(\n",
    "    classes_4_es,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_4_train, classes_4_test = train_test_split(\n",
    "    classes_4_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(len(classes_1_train), len(classes_1_dev), len(classes_1_test))\n",
    "print(len(classes_2_train), len(classes_2_dev), len(classes_2_test))\n",
    "print(len(classes_3_train), len(classes_3_dev), len(classes_3_test))\n",
    "print(len(classes_4_train), len(classes_4_dev), len(classes_4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_spt_prompt_en_assertions = WrapperEnglishSPT().activate_assertions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluate(\n",
    "    random.choices(classes_1_test, k=225)\n",
    "    + random.choices(classes_2_test, k=225)\n",
    "    + random.choices(classes_3_test, k=225)\n",
    "    + random.choices(classes_4_test, k=225),\n",
    "    evaluate_answer,\n",
    "    program_spt_prompt_en_assertions,\n",
    "    debug=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1cf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "\n",
    "# start_time = datetime.now()\n",
    "\n",
    "# teleprompter = MIPROv2(\n",
    "#     metric=evaluate_answer,\n",
    "#     task_model=lm,\n",
    "#     num_candidates=10,\n",
    "#     init_temperature=0.7,\n",
    "#     max_bootstrapped_demos=3,\n",
    "#     max_labeled_demos=4,\n",
    "#     verbose=False,\n",
    "# )\n",
    "\n",
    "# print(\"Optimizing program with MIPRO...\")\n",
    "# optimized_program = teleprompter.compile(\n",
    "#     program_spt_prompt_en_assertions.deepcopy(),\n",
    "#     trainset=random.choices(classes_1_train, k=500)\n",
    "#     + random.choices(classes_2_train, k=500)\n",
    "#     + random.choices(classes_3_train, k=500)\n",
    "#     + random.choices(classes_4_train, k=500),\n",
    "#     valset=random.choices(classes_1_dev, k=200)\n",
    "#     + random.choices(classes_2_dev, k=200)\n",
    "#     + random.choices(classes_3_dev, k=200)\n",
    "#     + random.choices(classes_4_dev, k=200),\n",
    "#     num_trials=15,\n",
    "#     minibatch_size=25,\n",
    "#     minibatch_full_eval_steps=10,\n",
    "#     minibatch=True,\n",
    "#     requires_permission_to_run=False,\n",
    "# )\n",
    "\n",
    "# optimized_program.save(f\"compile-models/sp/es_spt_mipro_optimized_prompt_en_llama3-3-q4\")\n",
    "\n",
    "# print(f\"Elapsed time: {datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85da4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# trial_logs = optimized_program.trial_logs\n",
    "\n",
    "# trial_numbers = list(trial_logs.keys())\n",
    "# scores = [trial_logs[trial][\"score\"] for trial in trial_numbers]\n",
    "\n",
    "# full_eval = [trial_logs[trial][\"full_eval\"] for trial in trial_numbers]\n",
    "\n",
    "# for trial_number, score, pruned in zip(trial_numbers, scores, full_eval):\n",
    "#     if pruned is False:\n",
    "#         plt.scatter(\n",
    "#             trial_number,\n",
    "#             score,\n",
    "#             color=\"grey\",\n",
    "#             label=(\n",
    "#                 \"Pruned Batch\"\n",
    "#                 if \"Pruned Batch\" not in plt.gca().get_legend_handles_labels()[1]\n",
    "#                 else \"\"\n",
    "#             ),\n",
    "#         )\n",
    "#     else:\n",
    "#         plt.scatter(\n",
    "#             trial_number,\n",
    "#             score,\n",
    "#             color=\"green\",\n",
    "#             label=(\n",
    "#                 \"Successful Batch\"\n",
    "#                 if \"Successful Batch\" not in plt.gca().get_legend_handles_labels()[1]\n",
    "#                 else \"\"\n",
    "#             ),\n",
    "#         )\n",
    "\n",
    "# plt.xlabel(\"Batch Number\")\n",
    "# plt.ylabel(\"Score\")\n",
    "# plt.title(\"Batch Scores\")\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score = 0\n",
    "# best_program_so_far = None\n",
    "\n",
    "\n",
    "# def get_signature(predictor):\n",
    "#     if hasattr(predictor, \"extended_signature\"):\n",
    "#         return predictor.extended_signature\n",
    "#     elif hasattr(predictor, \"signature\"):\n",
    "#         return predictor.signature\n",
    "\n",
    "\n",
    "# # print(f\"Baseline program | Score: {best_score}:\")\n",
    "# # for i, predictor in enumerate(WrapperEnglishSPT().predictors()):\n",
    "# #     print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "# # print()\n",
    "\n",
    "# print(\"----------------\")\n",
    "\n",
    "# for trial_num in optimized_program.trial_logs:\n",
    "#     program_score = optimized_program.trial_logs[trial_num][\"score\"]\n",
    "#     program_pruned = optimized_program.trial_logs[trial_num][\"full_eval\"]\n",
    "#     # if (\n",
    "#     #     program_score > best_score\n",
    "#     #     and program_pruned is True\n",
    "#     #     # and optimized_program.trial_logs[trial_num][\"full_eval\"]\n",
    "#     # ):\n",
    "#     if program_pruned is True:\n",
    "#         best_score = program_score\n",
    "#         best_program_so_far = optimized_program.trial_logs[trial_num][\"program\"]\n",
    "#     # if trial_num % 5 == 0:\n",
    "#     #     print(f\"Best program after {trial_num} batches | Score: {best_score}:\")\n",
    "#     #     for i, predictor in enumerate(best_program_so_far.predictors()):\n",
    "#     #         print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "#     #     print()\n",
    "    \n",
    "#         # print(f\"Best program with best score: {best_score}\")\n",
    "#         for i, predictor in enumerate(best_program_so_far.predictors()):\n",
    "#             print(f\"Prompt {trial_num} Instruction: {get_signature(predictor).instructions}\")\n",
    "#             print(best_score)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_spt_prompt_en_assertions.load(\n",
    "    \"compile-models/sp/es_spt_mipro_optimized_prompt_en_llama3-3-q4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3609a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_evaluate(\n",
    "    random.choices(classes_1_test, k=225)\n",
    "    + random.choices(classes_2_test, k=225)\n",
    "    + random.choices(classes_3_test, k=225)\n",
    "    + random.choices(classes_4_test, k=225),\n",
    "    evaluate_answer,\n",
    "    program_spt_prompt_en_assertions,\n",
    "    debug=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
