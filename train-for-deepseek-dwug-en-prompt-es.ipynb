{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78f12213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from typing import Literal, Union\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from programs import WrapperSpanishSPT, evaluate_answer\n",
    "from custom_evaluation import custom_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d929e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"ollama_chat/deepseek-r1:14b\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    ")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6618acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<think>\\n\\n</think>\\n\\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"What is your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82fa4184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5846, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"dev_dwug_en_resampled.csv\")\n",
    "display(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d04646",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    training_set.append(\n",
    "        dspy.Example(\n",
    "            sentence1=row[\"context_x\"],\n",
    "            sentence2=row[\"context_y\"],\n",
    "            target_word=row[\"lemma\"],\n",
    "            answer=int(row[\"judgment\"]),\n",
    "        ).with_inputs(\"sentence1\", \"sentence2\", \"target_word\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6854d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641\n",
      "1658\n",
      "1472\n",
      "2075\n",
      "409 129 103\n",
      "1060 332 266\n",
      "941 295 236\n",
      "1328 415 332\n"
     ]
    }
   ],
   "source": [
    "classes_1_en = [item for item in training_set if item.answer == 1]\n",
    "classes_2_en = [item for item in training_set if item.answer == 2]\n",
    "classes_3_en = [item for item in training_set if item.answer == 3]\n",
    "classes_4_en = [item for item in training_set if item.answer == 4]\n",
    "\n",
    "print(len(classes_1_en))\n",
    "print(len(classes_2_en))\n",
    "print(len(classes_3_en))\n",
    "print(len(classes_4_en))\n",
    "\n",
    "classes_1_train, classes_1_dev = train_test_split(\n",
    "    classes_1_en,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_1_train, classes_1_test = train_test_split(\n",
    "    classes_1_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "classes_2_train, classes_2_dev = train_test_split(\n",
    "    classes_2_en,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_2_train, classes_2_test = train_test_split(\n",
    "    classes_2_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "classes_3_train, classes_3_dev = train_test_split(\n",
    "    classes_3_en,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_3_train, classes_3_test = train_test_split(\n",
    "    classes_3_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "classes_4_train, classes_4_dev = train_test_split(\n",
    "    classes_4_en,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "classes_4_train, classes_4_test = train_test_split(\n",
    "    classes_4_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(len(classes_1_train), len(classes_1_dev), len(classes_1_test))\n",
    "print(len(classes_2_train), len(classes_2_dev), len(classes_2_test))\n",
    "print(len(classes_3_train), len(classes_3_dev), len(classes_3_test))\n",
    "print(len(classes_4_train), len(classes_4_dev), len(classes_4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_spt_prompt_es_assertions = WrapperSpanishSPT().activate_assertions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49818976",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluate(\n",
    "    random.choices(classes_1_test, k=100)\n",
    "    + random.choices(classes_2_test, k=100)\n",
    "    + random.choices(classes_3_test, k=100)\n",
    "    + random.choices(classes_4_test, k=100),\n",
    "    evaluate_answer,\n",
    "    program_spt_prompt_es_assertions,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57570bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "teleprompter = MIPROv2(\n",
    "    metric=evaluate_answer,\n",
    "    num_candidates=10,\n",
    "    init_temperature=0.7,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=4,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(\"Optimizing program with MIPRO...\")\n",
    "optimized_program = teleprompter.compile(\n",
    "    program_spt_prompt_es_assertions.deepcopy(),\n",
    "    trainset=random.choices(classes_1_train, k=400)\n",
    "    + random.choices(classes_2_train, k=400)\n",
    "    + random.choices(classes_3_train, k=400)\n",
    "    + random.choices(classes_4_train, k=400),\n",
    "    valset=random.choices(classes_1_dev, k=100)\n",
    "    + random.choices(classes_2_dev, k=100)\n",
    "    + random.choices(classes_3_dev, k=100)\n",
    "    + random.choices(classes_4_dev, k=100),\n",
    "    num_trials=15,\n",
    "    minibatch_size=25,\n",
    "    minibatch_full_eval_steps=10,\n",
    "    minibatch=True,\n",
    "    requires_permission_to_run=False,\n",
    ")\n",
    "\n",
    "optimized_program.save(f\"compile-models/sp/en_spt_mipro_optimized_prompt_es_deepseek-q4\")\n",
    "\n",
    "print(f\"Elapsed time: {datetime.now() - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "trial_logs = optimized_program.trial_logs\n",
    "\n",
    "trial_numbers = list(trial_logs.keys())\n",
    "scores = [trial_logs[trial][\"score\"] for trial in trial_numbers]\n",
    "\n",
    "full_eval = [trial_logs[trial][\"full_eval\"] for trial in trial_numbers]\n",
    "\n",
    "for trial_number, score, pruned in zip(trial_numbers, scores, full_eval):\n",
    "    if pruned is False:\n",
    "        plt.scatter(\n",
    "            trial_number,\n",
    "            score,\n",
    "            color=\"grey\",\n",
    "            label=(\n",
    "                \"Pruned Batch\"\n",
    "                if \"Pruned Batch\" not in plt.gca().get_legend_handles_labels()[1]\n",
    "                else \"\"\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        plt.scatter(\n",
    "            trial_number,\n",
    "            score,\n",
    "            color=\"green\",\n",
    "            label=(\n",
    "                \"Successful Batch\"\n",
    "                if \"Successful Batch\" not in plt.gca().get_legend_handles_labels()[1]\n",
    "                else \"\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Batch Scores\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17c2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "best_program_so_far = None\n",
    "\n",
    "\n",
    "def get_signature(predictor):\n",
    "    if hasattr(predictor, \"extended_signature\"):\n",
    "        return predictor.extended_signature\n",
    "    elif hasattr(predictor, \"signature\"):\n",
    "        return predictor.signature\n",
    "\n",
    "\n",
    "# print(f\"Baseline program | Score: {best_score}:\")\n",
    "# for i, predictor in enumerate(WrapperEnglishSPT().predictors()):\n",
    "#     print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "# print()\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "for trial_num in optimized_program.trial_logs:\n",
    "    program_score = optimized_program.trial_logs[trial_num][\"score\"]\n",
    "    program_pruned = optimized_program.trial_logs[trial_num][\"full_eval\"]\n",
    "    # if (\n",
    "    #     program_score > best_score\n",
    "    #     and program_pruned is True\n",
    "    #     # and optimized_program.trial_logs[trial_num][\"full_eval\"]\n",
    "    # ):\n",
    "    if program_pruned is True:\n",
    "        best_score = program_score\n",
    "        best_program_so_far = optimized_program.trial_logs[trial_num][\"program\"]\n",
    "    # if trial_num % 5 == 0:\n",
    "    #     print(f\"Best program after {trial_num} batches | Score: {best_score}:\")\n",
    "    #     for i, predictor in enumerate(best_program_so_far.predictors()):\n",
    "    #         print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "    #     print()\n",
    "    \n",
    "        # print(f\"Best program with best score: {best_score}\")\n",
    "        for i, predictor in enumerate(best_program_so_far.predictors()):\n",
    "            print(f\"Prompt {trial_num} Instruction: {get_signature(predictor).instructions}\")\n",
    "            print(best_score)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b32983",
   "metadata": {},
   "outputs": [],
   "source": [
    "program_spt_prompt_es_assertions.load(\n",
    "    \"compile-models/sp/en_spt_mipro_optimized_prompt_es_deepseek-q4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb2f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_evaluate(\n",
    "    random.choices(classes_1_test, k=100)\n",
    "    + random.choices(classes_2_test, k=100)\n",
    "    + random.choices(classes_3_test, k=100)\n",
    "    + random.choices(classes_4_test, k=100),\n",
    "    evaluate_answer,\n",
    "    program_spt_prompt_es_assertions,\n",
    "    debug=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
